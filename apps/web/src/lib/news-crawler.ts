// æ–°é—»é‡‡é›†æœåŠ¡
import { NewsSource, NewsArticle, CrawlJob } from '@/types/news'
import { createNews } from '@/data/news'
import { createMultiLanguageContent } from '@/lib/i18n'

// é‡‡é›†ç»Ÿè®¡æ¥å£
interface CrawlStats {
  totalFound: number
  totalProcessed: number
  totalSuccess: number
  totalFailed: number
  totalDuplicate: number
  totalFiltered: number
  errors: string[]
  duration: number
}

// ç®€å•çš„XMLè§£æå‡½æ•°
function parseXMLToObject(xmlString: string): any {
  // ç§»é™¤XMLå£°æ˜å’Œå‘½åç©ºé—´
  const cleanXml = xmlString
    .replace(/<\?xml[^>]*\?>/g, '')
    .replace(/xmlns[^=]*="[^"]*"/g, '')
    .trim()

  // ç®€å•çš„RSSè§£æ
  const result: any = {}

  // æå–channelä¿¡æ¯
  const channelMatch = cleanXml.match(/<channel[^>]*>([\s\S]*?)<\/channel>/i)
  if (channelMatch) {
    const channelContent = channelMatch[1]

    // æå–æ‰€æœ‰item
    const itemMatches = channelContent.match(/<item[^>]*>([\s\S]*?)<\/item>/gi)
    if (itemMatches) {
      result.items = itemMatches.map(itemXml => {
        const item: any = {}

        // æå–æ ‡é¢˜
        const titleMatch = itemXml.match(/<title[^>]*>([\s\S]*?)<\/title>/i)
        if (titleMatch) {
          item.title = titleMatch[1].replace(/<!\[CDATA\[([\s\S]*?)\]\]>/g, '$1').trim()
        }

        // æå–é“¾æ¥
        const linkMatch = itemXml.match(/<link[^>]*>([\s\S]*?)<\/link>/i)
        if (linkMatch) {
          item.link = linkMatch[1].replace(/<!\[CDATA\[([\s\S]*?)\]\]>/g, '$1').trim()
        }

        // æå–æè¿°
        const descMatch = itemXml.match(/<description[^>]*>([\s\S]*?)<\/description>/i)
        if (descMatch) {
          item.description = descMatch[1].replace(/<!\[CDATA\[([\s\S]*?)\]\]>/g, '$1').trim()
        }

        // æå–å‘å¸ƒæ—¥æœŸ
        const pubDateMatch = itemXml.match(/<pubDate[^>]*>([\s\S]*?)<\/pubDate>/i)
        if (pubDateMatch) {
          item.pubDate = pubDateMatch[1].replace(/<!\[CDATA\[([\s\S]*?)\]\]>/g, '$1').trim()
        }

        // æå–ä½œè€…
        const authorMatch = itemXml.match(/<(?:author|dc:creator)[^>]*>([\s\S]*?)<\/(?:author|dc:creator)>/i)
        if (authorMatch) {
          item.author = authorMatch[1].replace(/<!\[CDATA\[([\s\S]*?)\]\]>/g, '$1').trim()
        }

        // æå–å†…å®¹
        const contentMatch = itemXml.match(/<(?:content:encoded|content)[^>]*>([\s\S]*?)<\/(?:content:encoded|content)>/i)
        if (contentMatch) {
          item.content = contentMatch[1].replace(/<!\[CDATA\[([\s\S]*?)\]\]>/g, '$1').trim()
        }

        return item
      })
    }
  }

  return result
}

// å†…å®¹è´¨é‡è¯„ä¼°å™¨
export class ContentQualityAssessor {
  // å¢å¼ºçš„è´¨é‡è¯„ä¼° (0-100åˆ†) - æ›´ä¸¥æ ¼çš„æœè£…è¡Œä¸šæ ‡å‡†
  assessQuality(title: string, content: string, source?: NewsSource): number {
    let score = 40 // é™ä½åŸºç¡€åˆ†æ•°ï¼Œæ›´ä¸¥æ ¼

    // æ ‡é¢˜è´¨é‡è¯„ä¼° (25åˆ†)
    if (title) {
      const titleLength = title.length
      if (titleLength >= 15 && titleLength <= 120) score += 15
      if (titleLength >= 25 && titleLength <= 80) score += 5

      // æœè£…è¡Œä¸šå…³é”®è¯æ£€æŸ¥
      const fashionKeywords = [
        'fashion', 'textile', 'apparel', 'clothing', 'fabric', 'design', 'style', 'trend',
        'garment', 'manufacturing', 'sourcing', 'supply chain', 'retail', 'wholesale',
        'cotton', 'polyester', 'silk', 'wool', 'denim', 'knit', 'woven', 'fiber',
        'underwear', 'lingerie', 'intimate', 'bra', 'panty', 'shapewear'
      ]
      const keywordMatches = fashionKeywords.filter(keyword =>
        title.toLowerCase().includes(keyword)
      ).length
      score += Math.min(10, keywordMatches * 3)

      // é¿å…åƒåœ¾æ ‡é¢˜
      const spamIndicators = ['click here', 'free', 'urgent', 'limited time', '!!!', 'buy now']
      const hasSpam = spamIndicators.some(spam =>
        title.toLowerCase().includes(spam)
      )
      if (hasSpam) score -= 25
    }

    // å†…å®¹è´¨é‡è¯„ä¼° (35åˆ†)
    if (content) {
      const contentLength = content.length
      if (contentLength >= 300) score += 10
      if (contentLength >= 600) score += 10
      if (contentLength >= 1200) score += 10
      if (contentLength < 150) score -= 20

      // å†…å®¹ç»“æ„è¯„ä¼°
      const paragraphs = content.split('\n').filter(p => p.trim().length > 0)
      if (paragraphs.length >= 3) score += 5
      if (paragraphs.length >= 5) score += 5

      // B2Bå•†ä¸šå†…å®¹å…³é”®è¯
      const businessKeywords = [
        'manufacturing', 'production', 'export', 'import', 'trade', 'market',
        'industry', 'business', 'company', 'factory', 'supplier', 'buyer'
      ]
      const businessMatches = businessKeywords.filter(keyword =>
        content.toLowerCase().includes(keyword)
      ).length
      score += Math.min(10, businessMatches * 2)

      // æ£€æŸ¥é‡å¤å†…å®¹
      const sentences = content.split(/[.!?ã€‚ï¼ï¼Ÿ]/).filter(s => s.trim().length > 10)
      const uniqueSentences = new Set(sentences.map(s => s.trim().toLowerCase()))
      if (sentences.length > 0) {
        const uniqueRatio = uniqueSentences.size / sentences.length
        score += Math.round(uniqueRatio * 10)
        if (uniqueRatio < 0.7) score -= 15 // é‡å¤å†…å®¹è¿‡å¤šæ‰£åˆ†
      }

      // é¿å…åƒåœ¾å†…å®¹
      const contentSpamIndicators = [
        'click here', 'buy now', 'limited time', 'free shipping', 'call now',
        'ç‚¹å‡»æŸ¥çœ‹', 'æ›´å¤šè¯¦æƒ…', 'å¹¿å‘Š', 'æ¨å¹¿', 'è”ç³»æˆ‘ä»¬'
      ]
      const spamCount = contentSpamIndicators.filter(spam =>
        content.toLowerCase().includes(spam.toLowerCase())
      ).length
      score -= spamCount * 8
    }

    // æ¥æºè´¨é‡åŠ æƒ (10åˆ†)
    if (source?.filters?.minQualityScore) {
      const sourceQuality = source.filters.minQualityScore
      if (sourceQuality >= 85) score += 10
      else if (sourceQuality >= 75) score += 7
      else if (sourceQuality >= 65) score += 5
    }

    // ä¸“ä¸šæœ¯è¯­åŠ åˆ† (10åˆ†)
    const technicalTerms = [
      'sustainability', 'eco-friendly', 'organic', 'recycled', 'biodegradable',
      'automation', 'digitalization', 'quality control', 'certification'
    ]
    const technicalMatches = technicalTerms.filter(term =>
      (title + ' ' + content).toLowerCase().includes(term)
    ).length
    score += Math.min(10, technicalMatches * 2)

    return Math.max(0, Math.min(100, Math.round(score)))
  }

  // æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºé«˜è´¨é‡
  isHighQuality(title: string, content: string): boolean {
    return this.assessQuality(title, content) >= 70
  }
}

// æ™ºèƒ½å»é‡å™¨
export class DuplicateDetector {
  private existingTitles = new Set<string>()
  private existingUrls = new Set<string>()
  private titleSimilarityThreshold = 0.8
  private initialized = false

  // åˆå§‹åŒ–æ£€æµ‹å™¨ï¼Œä»æ•°æ®åº“åŠ è½½å·²å­˜åœ¨çš„æ–°é—»
  async initialize(): Promise<void> {
    if (this.initialized) return

    try {
      // åŠ¨æ€å¯¼å…¥æ•°æ®æ¨¡å—ä»¥é¿å…å¾ªç¯ä¾èµ–
      const { getAllNews } = await import('@/data/news')
      const existingNews = getAllNews()

      // åŠ è½½æ‰€æœ‰å·²å­˜åœ¨çš„æ ‡é¢˜å’ŒURL
      for (const news of existingNews) {
        if (typeof news.title === 'string') {
          this.existingTitles.add(news.title.toLowerCase())
        } else if (news.title && typeof news.title === 'object') {
          // å¤„ç†å¤šè¯­è¨€æ ‡é¢˜
          Object.values(news.title).forEach(title => {
            if (typeof title === 'string') {
              this.existingTitles.add(title.toLowerCase())
            }
          })
        }

        if (news.sourceUrl) {
          this.existingUrls.add(news.sourceUrl)
        }
      }

      this.initialized = true
      console.log(`é‡å¤æ£€æµ‹å™¨å·²åˆå§‹åŒ–: ${this.existingTitles.size} ä¸ªæ ‡é¢˜, ${this.existingUrls.size} ä¸ªURL`)
    } catch (error) {
      console.error('åˆå§‹åŒ–é‡å¤æ£€æµ‹å™¨å¤±è´¥:', error)
    }
  }

  // æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤å†…å®¹
  async isDuplicate(title: string, url: string, content?: string): Promise<boolean> {
    // ç¡®ä¿å·²åˆå§‹åŒ–
    await this.initialize()

    // URLå®Œå…¨åŒ¹é…
    if (this.existingUrls.has(url)) {
      return true
    }

    // æ ‡é¢˜å®Œå…¨åŒ¹é…
    if (this.existingTitles.has(title.toLowerCase())) {
      return true
    }

    // æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æŸ¥
    for (const existingTitle of this.existingTitles) {
      if (this.calculateSimilarity(title.toLowerCase(), existingTitle) > this.titleSimilarityThreshold) {
        return true
      }
    }

    return false
  }

  // æ·»åŠ åˆ°å·²çŸ¥å†…å®¹
  addContent(title: string, url: string): void {
    this.existingTitles.add(title.toLowerCase())
    this.existingUrls.add(url)
  }

  // è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ (ç®€å•çš„Jaccardç›¸ä¼¼åº¦)
  private calculateSimilarity(str1: string, str2: string): number {
    const words1 = new Set(str1.split(/\s+/))
    const words2 = new Set(str2.split(/\s+/))

    const intersection = new Set([...words1].filter(x => words2.has(x)))
    const union = new Set([...words1, ...words2])

    return intersection.size / union.size
  }

  // é‡ç½®æ£€æµ‹å™¨ï¼ˆä½†ä¿ç•™æ•°æ®åº“ä¸­çš„æ•°æ®ï¼‰
  reset(): void {
    // ä¸å†æ¸…ç©ºæ‰€æœ‰æ•°æ®ï¼Œåªé‡ç½®åˆå§‹åŒ–çŠ¶æ€
    this.initialized = false
  }
}

// å¢å¼ºçš„RSSè§£æå™¨
export class RSSCrawler {
  private qualityAssessor = new ContentQualityAssessor()
  private duplicateDetector = new DuplicateDetector()

  async crawl(source: NewsSource): Promise<{ articles: NewsArticle[], stats: CrawlStats }> {
    const startTime = Date.now()
    const stats: CrawlStats = {
      totalFound: 0,
      totalProcessed: 0,
      totalSuccess: 0,
      totalFailed: 0,
      totalDuplicate: 0,
      totalFiltered: 0,
      errors: [],
      duration: 0
    }

    try {
      console.log(`å¼€å§‹é‡‡é›†RSSæº: ${source.name}`)

      // é‡ç½®å»é‡æ£€æµ‹å™¨
      this.duplicateDetector.reset()

      let xmlText: string

      // å¦‚æœæ˜¯æµ‹è¯•RSSæºï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
      if (source.url === 'https://httpbin.org/xml' || source.name.includes('æµ‹è¯•')) {
        xmlText = `<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>æµ‹è¯•RSSé¢‘é“</title>
    <description>è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•RSSé¢‘é“</description>
    <link>https://example.com</link>
    <item>
      <title>äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æœ€æ–°å‘å±•</title>
      <link>https://example.com/ai-development</link>
      <description>äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰äº†é‡å¤§çªç ´ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ ç­‰æ–¹é¢ã€‚è¿™äº›æŠ€æœ¯çš„å‘å±•æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚</description>
      <pubDate>Mon, 26 Jul 2025 16:00:00 GMT</pubDate>
      <author>ç§‘æŠ€è®°è€…</author>
    </item>
    <item>
      <title>é‡å­è®¡ç®—çš„å•†ä¸šåŒ–å‰æ™¯</title>
      <link>https://example.com/quantum-computing</link>
      <description>é‡å­è®¡ç®—æŠ€æœ¯æ­£åœ¨ä»å®éªŒå®¤èµ°å‘å•†ä¸šåº”ç”¨ï¼Œå¤šå®¶ç§‘æŠ€å…¬å¸æ­£åœ¨æŠ•èµ„è¿™ä¸€é¢†åŸŸã€‚ä¸“å®¶é¢„æµ‹ï¼Œé‡å­è®¡ç®—å°†åœ¨æœªæ¥åå¹´å†…å®ç°é‡å¤§å•†ä¸šçªç ´ã€‚</description>
      <pubDate>Mon, 26 Jul 2025 15:30:00 GMT</pubDate>
      <author>æŠ€æœ¯åˆ†æå¸ˆ</author>
    </item>
    <item>
      <title>5Gç½‘ç»œçš„å…¨çƒéƒ¨ç½²è¿›å±•</title>
      <link>https://example.com/5g-deployment</link>
      <description>5Gç½‘ç»œåœ¨å…¨çƒèŒƒå›´å†…çš„éƒ¨ç½²æ­£åœ¨åŠ é€Ÿï¼Œä¸ºç‰©è”ç½‘ã€è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½åŸå¸‚ç­‰åº”ç”¨æä¾›äº†åŸºç¡€è®¾æ–½æ”¯æŒã€‚</description>
      <pubDate>Mon, 26 Jul 2025 15:00:00 GMT</pubDate>
      <author>ç½‘ç»œä¸“å®¶</author>
    </item>
  </channel>
</rss>`
      } else {
        const controller = new AbortController()
        const timeoutId = setTimeout(() => controller.abort(), 10000) // 10ç§’è¶…æ—¶

        const response = await fetch(source.url, {
          signal: controller.signal,
          headers: {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/rss+xml, application/xml, text/xml',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cache-Control': 'no-cache'
          },
          timeout: 30000 // 30ç§’è¶…æ—¶
        })

        clearTimeout(timeoutId)

        if (!response.ok) {
          throw new Error(`HTTP ${response.status}: ${response.statusText}`)
        }

        xmlText = await response.text()
      }
      const { articles, parseStats } = await this.parseRSS(xmlText, source)

      // åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
      Object.assign(stats, parseStats)
      stats.duration = Date.now() - startTime

      console.log(`RSSé‡‡é›†å®Œæˆ: ${source.name}`)
      console.log(`- å‘ç°: ${stats.totalFound} ç¯‡`)
      console.log(`- æˆåŠŸ: ${stats.totalSuccess} ç¯‡`)
      console.log(`- é‡å¤: ${stats.totalDuplicate} ç¯‡`)
      console.log(`- è¿‡æ»¤: ${stats.totalFiltered} ç¯‡`)
      console.log(`- å¤±è´¥: ${stats.totalFailed} ç¯‡`)
      console.log(`- è€—æ—¶: ${stats.duration}ms`)

      return { articles, stats }

    } catch (error) {
      stats.duration = Date.now() - startTime
      stats.errors.push(error instanceof Error ? error.message : String(error))
      console.error(`RSSé‡‡é›†å¤±è´¥ [${source.name}]:`, error)
      throw error
    }
  }
  
  private async parseRSS(xmlText: string, source: NewsSource): Promise<{ articles: NewsArticle[], parseStats: Partial<CrawlStats> }> {
    const articles: NewsArticle[] = []
    const parseStats: Partial<CrawlStats> = {
      totalFound: 0,
      totalProcessed: 0,
      totalSuccess: 0,
      totalFailed: 0,
      totalDuplicate: 0,
      totalFiltered: 0,
      errors: []
    }

    try {
      // æ£€æµ‹XMLç¼–ç 
      const encoding = this.detectEncoding(xmlText)
      console.log(`æ£€æµ‹åˆ°ç¼–ç : ${encoding}`)

      // æ¸…ç†XMLå†…å®¹
      const cleanXml = this.cleanXmlContent(xmlText)

      // ä½¿ç”¨ç®€å•çš„XMLè§£æå™¨
      const xmlDoc = parseXMLToObject(cleanXml)

      // è·å–items
      const items = xmlDoc.items || []

      parseStats.totalFound = items.length

      console.log(`å‘ç° ${items.length} ä¸ªæ¡ç›®`)

      for (let i = 0; i < items.length; i++) {
        const item = items[i]
        parseStats.totalProcessed!++

        try {
          const articleData = this.extractArticleData(item, source)

          if (!articleData.title || !articleData.link) {
            parseStats.totalFailed!++
            parseStats.errors!.push(`æ¡ç›® ${i + 1}: ç¼ºå°‘æ ‡é¢˜æˆ–é“¾æ¥`)
            continue
          }

          // æ£€æŸ¥é‡å¤
          if (await this.duplicateDetector.isDuplicate(articleData.title, articleData.link, articleData.content)) {
            parseStats.totalDuplicate!++
            console.log(`è·³è¿‡é‡å¤æ–‡ç« : ${articleData.title}`)
            continue
          }

          // åº”ç”¨è¿‡æ»¤è§„åˆ™
          if (!this.passesFilters(articleData.title, articleData.content, source)) {
            parseStats.totalFiltered!++
            console.log(`è¿‡æ»¤æ–‡ç« : ${articleData.title}`)
            continue
          }

          // è´¨é‡è¯„ä¼°
          const qualityScore = this.qualityAssessor.assessQuality(articleData.title, articleData.content)
          console.log(`æ–‡ç« è´¨é‡è¯„åˆ†: ${qualityScore} - ${articleData.title}`)

          // ä½¿ç”¨åˆç†çš„è´¨é‡è¿‡æ»¤é˜ˆå€¼
          const minQualityScore = source.filters?.minQualityScore || 25 // è®¾ä¸º25åˆ†ï¼Œè¿‡æ»¤æ‰æ˜æ˜¾çš„ä½è´¨é‡å†…å®¹
          if (qualityScore < minQualityScore) {
            parseStats.totalFiltered!++
            console.log(`è´¨é‡ä¸è¾¾æ ‡: ${articleData.title} (${qualityScore} < ${minQualityScore})`)
            continue
          } else {
            console.log(`âœ… è´¨é‡è¾¾æ ‡: ${articleData.title} (${qualityScore} >= ${minQualityScore})`)
          }

          const article: Omit<NewsArticle, 'id' | 'createdAt' | 'updatedAt' | 'viewCount' | 'likeCount' | 'shareCount'> = {
            title: createMultiLanguageContent(articleData.title, source.language || 'zh'),
            originalTitle: articleData.title,
            content: createMultiLanguageContent(articleData.content, source.language || 'zh'),
            originalContent: articleData.content,
            summary: createMultiLanguageContent(this.generateSummary(articleData.content), source.language || 'zh'),
            category: source.category,
            status: 'published',
            sourceUrl: articleData.link,
            sourceName: source.name,
            sourceType: 'rss',
            slug: this.generateSlug(articleData.title),
            featuredImage: articleData.image,
            keywords: this.extractKeywords(articleData.title, articleData.content),
            aiProcessed: false,
            aiProcessStatus: 'pending',
            publishedAt: articleData.pubDate,
            author: articleData.author,
            qualityScore
          }

          articles.push(article)
          this.duplicateDetector.addContent(articleData.title, articleData.link)
          parseStats.totalSuccess!++

        } catch (error) {
          parseStats.totalFailed!++
          const errorMsg = `æ¡ç›® ${i + 1} å¤„ç†å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`
          parseStats.errors!.push(errorMsg)
          console.error(errorMsg)
        }
      }

    } catch (error) {
      console.error('RSSè§£æé”™è¯¯:', error)
      parseStats.errors!.push(`è§£æå¤±è´¥: ${error instanceof Error ? error.message : String(error)}`)
      throw error
    }

    return { articles, parseStats }
  }

  // æå–æ–‡ç« æ•°æ®
  private extractArticleData(item: any, source: NewsSource): {
    title: string
    link: string
    content: string
    pubDate?: Date
    author?: string
    image?: string
  } {
    // ä»æˆ‘ä»¬çš„ç®€å•è§£æå™¨è·å–æ•°æ®
    let title = item.title || ''
    let link = item.link || ''
    let description = item.description || ''
    let content = item.content || description
    let pubDate = item.pubDate
    let author = item.author || ''

    // å›¾ç‰‡æå– - ä»å†…å®¹ä¸­æå–ç¬¬ä¸€å¼ å›¾ç‰‡
    let image = ''
    if (content) {
      const imgMatch = content.match(/<img[^>]+src=["']([^"']+)["']/i)
      if (imgMatch) {
        image = imgMatch[1]
      }
    }

    // æ¸…ç†HTMLæ ‡ç­¾
    if (typeof content === 'string') {
      content = this.cleanHtmlContent(content)
    }

    return {
      title: this.cleanText(String(title)),
      link: String(link).trim(),
      content: this.cleanText(String(content)),
      pubDate: pubDate ? this.parseDate(String(pubDate)) : undefined,
      author: author ? this.cleanText(String(author)) : undefined,
      image: String(image).trim() || undefined
    }
  }
  
  // æ£€æµ‹XMLç¼–ç 
  private detectEncoding(xmlText: string): string {
    const encodingMatch = xmlText.match(/encoding=["']([^"']+)["']/i)
    return encodingMatch ? encodingMatch[1] : 'UTF-8'
  }

  // æ¸…ç†XMLå†…å®¹
  private cleanXmlContent(xmlText: string): string {
    // ç§»é™¤BOM
    xmlText = xmlText.replace(/^\uFEFF/, '')

    // ä¿®å¤å¸¸è§çš„XMLé—®é¢˜
    xmlText = xmlText.replace(/&(?!(?:amp|lt|gt|quot|apos);)/g, '&amp;')

    return xmlText
  }

  // æ¸…ç†HTMLå†…å®¹
  private cleanHtmlContent(html: string): string {
    if (!html) return ''

    // ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
    html = html.replace(/<script[^>]*>[\s\S]*?<\/script>/gi, '')
    html = html.replace(/<style[^>]*>[\s\S]*?<\/style>/gi, '')

    // ç§»é™¤HTMLæ ‡ç­¾ä½†ä¿ç•™æ¢è¡Œ
    html = html.replace(/<br\s*\/?>/gi, '\n')
    html = html.replace(/<\/p>/gi, '\n\n')
    html = html.replace(/<[^>]+>/g, '')

    // è§£ç HTMLå®ä½“
    html = html.replace(/&amp;/g, '&')
    html = html.replace(/&lt;/g, '<')
    html = html.replace(/&gt;/g, '>')
    html = html.replace(/&quot;/g, '"')
    html = html.replace(/&apos;/g, "'")
    html = html.replace(/&#(\d+);/g, (match, dec) => String.fromCharCode(dec))
    html = html.replace(/&#x([0-9a-f]+);/gi, (match, hex) => String.fromCharCode(parseInt(hex, 16)))

    return html
  }

  // æ¸…ç†æ–‡æœ¬
  private cleanText(text: string): string {
    if (!text) return ''

    return text
      .replace(/\s+/g, ' ') // åˆå¹¶å¤šä¸ªç©ºç™½å­—ç¬¦
      .replace(/\n\s*\n/g, '\n') // åˆå¹¶å¤šä¸ªæ¢è¡Œ
      .trim()
  }

  // è§£ææ—¥æœŸ
  private parseDate(dateStr: string): Date | undefined {
    try {
      // å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
      const date = new Date(dateStr)
      if (!isNaN(date.getTime())) {
        return date
      }

      // RFC 2822 æ ¼å¼
      const rfc2822Match = dateStr.match(/(\w{3}),?\s+(\d{1,2})\s+(\w{3})\s+(\d{4})\s+(\d{1,2}):(\d{2}):(\d{2})/)
      if (rfc2822Match) {
        return new Date(dateStr)
      }

      return undefined
    } catch {
      return undefined
    }
  }

  // æå–å…³é”®è¯
  private extractKeywords(title: string, content: string): string[] {
    const text = `${title} ${content}`.toLowerCase()
    const words = text.match(/[\u4e00-\u9fa5a-z]+/g) || []

    // ç»Ÿè®¡è¯é¢‘
    const wordCount = new Map<string, number>()
    words.forEach(word => {
      if (word.length >= 2) { // åªè€ƒè™‘é•¿åº¦>=2çš„è¯
        wordCount.set(word, (wordCount.get(word) || 0) + 1)
      }
    })

    // æŒ‰é¢‘ç‡æ’åºå¹¶å–å‰10ä¸ª
    return Array.from(wordCount.entries())
      .sort((a, b) => b[1] - a[1])
      .slice(0, 10)
      .map(([word]) => word)
  }

  private passesFilters(title: string, content: string, source: NewsSource): boolean {
    const filters = source.filters
    if (!filters) return true

    const text = `${title} ${content}`.toLowerCase()

    // æ£€æŸ¥å¿…é¡»åŒ…å«çš„å…³é”®è¯
    if (filters.keywords && filters.keywords.length > 0) {
      const hasRequiredKeyword = filters.keywords.some(keyword =>
        text.includes(keyword.toLowerCase())
      )
      if (!hasRequiredKeyword) return false
    }

    // æ£€æŸ¥æ’é™¤çš„å…³é”®è¯
    if (filters.excludeKeywords && filters.excludeKeywords.length > 0) {
      const hasExcludedKeyword = filters.excludeKeywords.some(keyword => 
        text.includes(keyword.toLowerCase())
      )
      if (hasExcludedKeyword) return false
    }
    
    // æ£€æŸ¥å†…å®¹é•¿åº¦
    if (filters.minContentLength && content.length < filters.minContentLength) {
      return false
    }
    
    if (filters.maxContentLength && content.length > filters.maxContentLength) {
      return false
    }
    
    return true
  }
  
  private generateSummary(content: string): string {
    // ç®€å•çš„æ‘˜è¦ç”Ÿæˆï¼šå–å‰150ä¸ªå­—ç¬¦
    const cleanContent = content.replace(/<[^>]*>/g, '').trim()
    return cleanContent.length > 150 
      ? cleanContent.substring(0, 150) + '...'
      : cleanContent
  }
  
  private generateSlug(title: string): string {
    return title
      .toLowerCase()
      .replace(/[^\w\s-]/g, '')
      .replace(/\s+/g, '-')
      .substring(0, 50)
  }
}

// ç½‘é¡µçˆ¬è™«
export class WebScraper {
  async crawl(source: NewsSource): Promise<NewsArticle[]> {
    try {
      console.log(`å¼€å§‹çˆ¬å–ç½‘é¡µ: ${source.name}`)
      
      const controller = new AbortController()
      const timeoutId = setTimeout(() => controller.abort(), 8000) // 8ç§’è¶…æ—¶

      const response = await fetch(source.url, {
        signal: controller.signal,
        headers: {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
      })

      clearTimeout(timeoutId)
      
      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`)
      }
      
      const html = await response.text()
      const articles = await this.parseHTML(html, source)
      
      console.log(`ç½‘é¡µçˆ¬å–å®Œæˆï¼Œè·å–åˆ° ${articles.length} ç¯‡æ–‡ç« `)
      return articles
      
    } catch (error) {
      console.error(`ç½‘é¡µçˆ¬å–å¤±è´¥ [${source.name}]:`, error)
      throw error
    }
  }
  
  private async parseHTML(html: string, source: NewsSource): Promise<NewsArticle[]> {
    const articles: NewsArticle[] = []
    const config = source.scrapingConfig
    
    if (!config) {
      throw new Error('ç¼ºå°‘çˆ¬è™«é…ç½®')
    }
    
    try {
      // ä½¿ç”¨DOMParserè§£æHTML
      const parser = new DOMParser()
      const doc = parser.parseFromString(html, 'text/html')
      
      // æŸ¥æ‰¾æ–‡ç« é“¾æ¥
      const articleLinks = doc.querySelectorAll('a[href]')
      const processedUrls = new Set<string>()
      
      for (const link of articleLinks) {
        const href = link.getAttribute('href')
        if (!href || processedUrls.has(href)) continue
        
        // æ„å»ºå®Œæ•´URL
        const fullUrl = href.startsWith('http') ? href : new URL(href, source.url).href
        processedUrls.add(fullUrl)
        
        try {
          // è·å–æ–‡ç« è¯¦æƒ…é¡µ
          const articleController = new AbortController()
          const articleTimeoutId = setTimeout(() => articleController.abort(), 5000) // 5ç§’è¶…æ—¶

          const articleResponse = await fetch(fullUrl, {
            signal: articleController.signal,
            headers: {
              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
          })

          clearTimeout(articleTimeoutId)
          
          if (!articleResponse.ok) continue
          
          const articleHtml = await articleResponse.text()
          const articleDoc = parser.parseFromString(articleHtml, 'text/html')
          
          // æå–æ–‡ç« ä¿¡æ¯
          const title = this.extractText(articleDoc, config.titleSelector)
          const content = this.extractText(articleDoc, config.contentSelector)
          const summary = config.summarySelector 
            ? this.extractText(articleDoc, config.summarySelector)
            : this.generateSummary(content)
          const author = config.authorSelector 
            ? this.extractText(articleDoc, config.authorSelector)
            : undefined
          const image = config.imageSelector
            ? this.extractAttribute(articleDoc, config.imageSelector, 'src')
            : undefined
          
          if (!title || !content) continue
          
          // åº”ç”¨è¿‡æ»¤è§„åˆ™
          if (!this.passesFilters(title, content, source)) {
            continue
          }
          
          const article: Omit<NewsArticle, 'id' | 'createdAt' | 'updatedAt' | 'viewCount' | 'likeCount' | 'shareCount'> = {
            title: createMultiLanguageContent(title, source.language || 'zh'),
            originalTitle: title,
            content: createMultiLanguageContent(content, source.language || 'zh'),
            originalContent: content,
            summary: createMultiLanguageContent(summary, source.language || 'zh'),
            category: source.category,
            status: 'published',
            sourceUrl: fullUrl,
            sourceName: source.name,
            sourceType: 'web_scraping',
            featuredImage: image,
            slug: this.generateSlug(title),
            aiProcessed: false,
            aiProcessStatus: 'pending',
            author
          }
          
          articles.push(article)
          
          // é™åˆ¶é‡‡é›†æ•°é‡ï¼Œé¿å…è¿‡åº¦è¯·æ±‚
          if (articles.length >= 10) break
          
        } catch (error) {
          console.error(`å¤„ç†æ–‡ç« å¤±è´¥ [${fullUrl}]:`, error)
          continue
        }
        
        // æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        await new Promise(resolve => setTimeout(resolve, 1000))
      }
      
    } catch (error) {
      console.error('HTMLè§£æé”™è¯¯:', error)
      throw new Error('HTMLè§£æå¤±è´¥')
    }
    
    return articles
  }
  
  private extractText(doc: Document, selector: string): string {
    const element = doc.querySelector(selector)
    return element?.textContent?.trim() || ''
  }
  
  private extractAttribute(doc: Document, selector: string, attribute: string): string | undefined {
    const element = doc.querySelector(selector)
    return element?.getAttribute(attribute) || undefined
  }
  
  private passesFilters(title: string, content: string, source: NewsSource): boolean {
    const filters = source.filters
    if (!filters) return true
    
    const text = `${title} ${content}`.toLowerCase()
    
    // æ£€æŸ¥å¿…é¡»åŒ…å«çš„å…³é”®è¯
    if (filters.keywords && filters.keywords.length > 0) {
      const hasRequiredKeyword = filters.keywords.some(keyword => 
        text.includes(keyword.toLowerCase())
      )
      if (!hasRequiredKeyword) return false
    }
    
    // æ£€æŸ¥æ’é™¤çš„å…³é”®è¯
    if (filters.excludeKeywords && filters.excludeKeywords.length > 0) {
      const hasExcludedKeyword = filters.excludeKeywords.some(keyword => 
        text.includes(keyword.toLowerCase())
      )
      if (hasExcludedKeyword) return false
    }
    
    // æ£€æŸ¥å†…å®¹é•¿åº¦
    if (filters.minContentLength && content.length < filters.minContentLength) {
      return false
    }
    
    if (filters.maxContentLength && content.length > filters.maxContentLength) {
      return false
    }
    
    return true
  }
  
  private generateSummary(content: string): string {
    // ç®€å•çš„æ‘˜è¦ç”Ÿæˆï¼šå–å‰150ä¸ªå­—ç¬¦
    const cleanContent = content.replace(/<[^>]*>/g, '').trim()
    return cleanContent.length > 150 
      ? cleanContent.substring(0, 150) + '...'
      : cleanContent
  }
  
  private generateSlug(title: string): string {
    return title
      .toLowerCase()
      .replace(/[^\w\s-]/g, '')
      .replace(/\s+/g, '-')
      .substring(0, 50)
  }
}

// æ–°é—»é‡‡é›†ç®¡ç†å™¨
export class NewsCrawlerManager {
  private rssCrawler = new RSSCrawler()
  private webScraper = new WebScraper()
  
  async crawlSource(source: NewsSource): Promise<CrawlJob> {
    const job: CrawlJob = {
      id: Date.now().toString(),
      sourceId: source.id,
      status: 'running',
      startedAt: new Date(),
      totalFound: 0,
      totalProcessed: 0,
      totalSuccess: 0,
      totalFailed: 0,
      errors: [],
      createdAt: new Date()
    }

    try {
      console.log(`å¼€å§‹é‡‡é›†æ–°é—»æº: ${source.name} (${source.type})`)
      let crawlResult: { articles: NewsArticle[], stats: CrawlStats }
      
      switch (source.type) {
        case 'rss':
          crawlResult = await this.rssCrawler.crawl(source)
          break
        case 'web_scraping':
          // TODO: æ›´æ–°WebScraperä»¥è¿”å›ç»Ÿè®¡ä¿¡æ¯
          const webArticles = await this.webScraper.crawl(source)
          crawlResult = {
            articles: webArticles,
            stats: {
              totalFound: webArticles.length,
              totalProcessed: webArticles.length,
              totalSuccess: webArticles.length,
              totalFailed: 0,
              totalDuplicate: 0,
              totalFiltered: 0,
              errors: [],
              duration: 0
            }
          }
          break
        default:
          throw new Error(`ä¸æ”¯æŒçš„é‡‡é›†ç±»å‹: ${source.type}`)
      }
      
      // æ›´æ–°jobç»Ÿè®¡ä¿¡æ¯
      job.totalFound = crawlResult.stats.totalFound
      job.totalProcessed = crawlResult.stats.totalProcessed

      console.log(`ğŸ” é‡‡é›†ç»“æœåˆ†æ:`)
      console.log(`- crawlResult.articles.length: ${crawlResult.articles.length}`)
      console.log(`- crawlResult.stats:`, crawlResult.stats)

      // åº”ç”¨æœ€å¤§æ–‡ç« æ•°é™åˆ¶
      const maxArticles = source.filters?.maxArticlesPerCrawl || 50
      const articlesToSave = crawlResult.articles.slice(0, maxArticles)

      // å…ˆæ”¶é›†æ‰€æœ‰æ–‡ç« ï¼Œä¸ç«‹å³ä¿å­˜åˆ°æ•°æ®åº“
      job.articlesData = articlesToSave
      job.totalSuccess = articlesToSave.length
      job.totalFailed = crawlResult.stats.totalFailed

      console.log(`âœ… é‡‡é›†å™¨è®¾ç½® articlesData: ${job.articlesData.length} ç¯‡æ–‡ç« `)
      console.log(`âœ… job.totalSuccess: ${job.totalSuccess}`)
      console.log(`âœ… job.totalFailed: ${job.totalFailed}`)

      // æ·»åŠ é‡‡é›†ç»Ÿè®¡ä¿¡æ¯åˆ°é”™è¯¯æ—¥å¿—ï¼ˆç”¨äºè°ƒè¯•ï¼‰
      if (crawlResult.stats.totalDuplicate > 0) {
        job.errors?.push(`è·³è¿‡é‡å¤æ–‡ç« : ${crawlResult.stats.totalDuplicate} ç¯‡`)
      }
      if (crawlResult.stats.totalFiltered > 0) {
        job.errors?.push(`è¿‡æ»¤ä½è´¨é‡æ–‡ç« : ${crawlResult.stats.totalFiltered} ç¯‡`)
      }

      console.log(`é‡‡é›†å®Œæˆ: ${source.name}`)
      console.log(`- å‘ç°: ${job.totalFound} ç¯‡`)
      console.log(`- æ”¶é›†æˆåŠŸ: ${job.totalSuccess} ç¯‡`)
      console.log(`- articlesDataé•¿åº¦: ${job.articlesData?.length || 0} ç¯‡`)
      console.log(`- ä¿å­˜å¤±è´¥: ${failedCount} ç¯‡`)
      console.log(`- é‡å¤è·³è¿‡: ${crawlResult.stats.totalDuplicate} ç¯‡`)
      console.log(`- è´¨é‡è¿‡æ»¤: ${crawlResult.stats.totalFiltered} ç¯‡`)
      
      job.status = 'completed'
      job.completedAt = new Date()
      
    } catch (error) {
      job.status = 'failed'
      job.completedAt = new Date()
      job.errors?.push(`é‡‡é›†å¤±è´¥: ${error}`)
    }
    
    return job
  }
  
  async crawlAllActiveSources(): Promise<CrawlJob[]> {
    const { getActiveSources } = await import('@/data/news')
    const sources = getActiveSources()
    const jobs: CrawlJob[] = []
    
    for (const source of sources) {
      try {
        const job = await this.crawlSource(source)
        jobs.push(job)
      } catch (error) {
        console.error(`é‡‡é›†æºå¤±è´¥ [${source.name}]:`, error)
      }
    }
    
    return jobs
  }
}
